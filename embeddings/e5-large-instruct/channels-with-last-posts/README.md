
# Channel's embeddings with last posts

В этой папке описан пайплайн для вычисления эмбеддингов постов по информации о прошлых постах из канала.

Просто сконкатенировать посты не получится - не влезет в модель. Значит надо либо:

1. суммаризовать сконкатенированный пост другой моделькой (например мистралью (large) - там большое окно (128k) + фри апи)
2. пулинг
3. либо юзать модель с большим контекстным окном чтобы туда влезало это всё - например `deepvk/USER-bge-m3` - контекстное окно 8к - по идее туда может влезть много чего

## Подход с суммаразацией

1. `SELECT id FROM parse.posts_content WHERE raw_text IS NOT NULL AND raw_text <> '';`
2. `SELECT id, channel_id, post_date FROM parse.post_metadata;`
3. `SELECT id, channel_id, post_date FROM ml_house.final_basis_with_metrics_v2;`

На основе этих таблиц [с помощью этого скрипта](./find_ids_of_prev_posts_for_ad_posts.ipynb) рассчитываем отображение от `id поста с рекламой` к `N id постов (содержащих текст) предыдущих по времени к этому рекламному посту`.

Далее, с помощью [скрипта](./load_prev_posts_texts.ipynb) скачиваем все текста прошлых постов по отношению к рекламному.

[Анализ длины](./analyze_prev_posts_texts.ipynb)

Суммаризация с помощью [`mistral-large-latest`](https://docs.mistral.ai/getting-started/models/models_overview/#premier-models):

* [промпт](./summary_prompt.txt)
* [Скрипт для создания саммари с помощью Mistal large](./summarization.py). [Пример результата его работы](./example_summary_output.json).
* ВРЕМЯ: 10к рекламных постов обрабатывается за 12 часов (т.е. для создания 10к саммари по предыдущим 10 постам перед таргетным рекламным). Всего у нас пока что 177к рекламных постов, значит ~216 часов, но вообще говоря можно параллеить на разные MISTRAL_AI - у меня например есть 5 таких - значит за 2-3 дня смогу обработать все посты.

Далее, с помощью [скрипта](./ch_sum_emb_w_e5_instruct.ipynb) высчитываем эмбеды каналов по его предшествующим рекламному постам.

* сконкатенировал название, описание канала и суммаризацию по постам (т.е. не исопльзовал текст закрепленного поста)

[Сохранение в `embeddings.summary_channel_emb`](./ch_sum_emb_e5_instruct_load_to_db.ipynb)

**PS**: пока что не залил в бд эти эмбеды, т.к. не закончил считать суммаризацию.
